# -*- coding: utf-8 -*-

import gc
from collections import Counter, defaultdict
from math import log
from operator import itemgetter
from string import punctuation

import pandas as pd
from sklearn.feature_extraction import text
from zhon import hanzi

from .arsenal_stats import sort_dic, split_dic


########################################################################################################################


def cut_pos(possed_word):
    """
    :param possed_word: word#pos
    """
    return possed_word.split('#')[0]


def get_lexical(possed_sen, pos_tuple):
    """
    extract words with specific pos-tags from a tagged sentence.
    :param possed_sen: list, a pos-tagged sentence
    :param pos_tuple: a tuple of pos-tags, eg: (#NN, #VV)
    :return a generator of words with appointed tags
    """
    return (i.split('#')[0] for i in possed_sen.split('\s') if i.endswith(pos_tuple))


def remove_punc(line):
    return ''.join(i.lower().strip(punctuation + hanzi.punctuation + 'â€œ') for i in line if not i.isnumeric())


########################################################################################################################


# Document Processing


def get_tfidf(corpus):
    """
    :param corpus: a list of space-separated strings
    :return a dict of {word: score} pair
    """
    tf, df = Counter(), Counter()
    for line in corpus:
        words = line.split()
        tf.update(words)
        df.update(set(words))
    return {word: 1.0 * freq / df[word] for word, freq in iter(tf.items())}


def get_tfdf(corpus):
    """
    :param corpus: a list of space-separated strings
    :return a dict of {word: score} pair
    """
    tf, df, count = Counter(), Counter(), []
    for line in corpus:
        words = line.split()
        tf.update(words)
        df.update(set(words))
        count.append(len(words))
    tfdf = {word: 1.0 * freq * df[word] for word, freq in iter(tf.items())}
    gc.collect()
    return tf, tfdf, sum(count)


def skl_tfidf(corpus):
    """
    use sklearn.feature_extraction.TfidfVectorizer to calculate tfidf
    :param corpus a list of strings
    """
    tt = text.TfidfVectorizer(token_pattern=u'(?u)\\b\\w\\b')
    cc = tt.fit_transform(corpus)
    words_array, tfidf_array = tt.get_feature_names(), cc.toarray()
    return words_array, tfidf_array


def split_tfidt(word_list, tfidf_dic, parts=5):
    """
    split a tfidf dict into 20/80
    :param word_list: a list of words
    :param tfidf_dic: k: word, v: tfidf
    :param parts: the number of parts
    """
    word_tfidf = sorted({(word, tfidf_dic[word]) for word in word_list}, key=itemgetter(1), reverse=True)
    length = len(word_tfidf)
    delimiter = length / parts

    if length > (parts - 1):
        first_part = split_dic(word_tfidf, end=delimiter)
        last_part = split_dic(word_tfidf, start=delimiter)
        word_tfidf_str = '\t'.join((first_part, last_part))
    else:
        word_tfidf_str = split_dic(word_tfidf, precision=3)

    return word_tfidf_str


def count_word(lst, rev=True):
    """
    :param lst: a list of word strings
    :param rev: reverse
    :return a sorted keyword list
    """
    return sorted(Counter(lst).items(), key=itemgetter(1), reverse=rev)


def count_10pct(lst):
    """
    :param lst: a list of int values:
    :return a list of every 10 pct point of the input line
    """
    if len(lst) < 11:
        return lst
    else:
        points = range(0, len(lst), len(lst) / 10)
        result = [lst[i] for i in points]
        return result if result[-1] == max(lst) else result + [max(lst)]


def cal_entropy(prob_list):
    """
    :param prob_list: a list of word probability by each category where the sum is 1.0.
    """
    return -1.0 * sum((i * log(i) if i else 0) for i in prob_list)


def prepare_entropy(corpus, delimiter=','):
    """
    :param corpus: 'delimiter'.join((brand, topic, dimension, weight))
    :param delimiter: delimiter
    """
    keyword_sum_dic, keyword_brand_dic, topic_brand_prob_dic = defaultdict(list), defaultdict(int), defaultdict(list)

    for line in corpus:
        line = line.strip().split(delimiter)
        keyword_sum_dic[(line[1], line[2])].append(int(line[3]))
        keyword_brand_dic[((line[1], line[2]), line[0])] = int(line[3])

    keyword_sum_dic = {k: sum(v) for k, v in keyword_sum_dic.items()}

    for k, v in keyword_brand_dic.items():
        topic_brand_prob_dic[k[0]].append((k[1], 1.0 * v / keyword_sum_dic[k[0]]))
    return topic_brand_prob_dic, keyword_sum_dic


def cal_brand_entropy(topic_brand_prob_dic):
    """
    :param topic_brand_prob_dic: a dic generated by prepare_entropy
    """
    topic_entropy_dic = defaultdict(float)
    for keyword, brand_freq in topic_brand_prob_dic.items():
        freq_list = [i[1] for i in brand_freq]
        topic_entropy_dic[keyword] = cal_entropy(freq_list)
    return topic_entropy_dic


def clean_pos_dic(dic, stop_list):
    """
    :param dic: {word#pos: value}
    :param stop_list: [stop words]
    :return: a cleaned dic
    """
    dic = {k: v for k, v in dic.items() for char in k if u'\u4e00' <= char <= u'\u9fff'}
    dic = {k: v for k, v in dic.items() if '#N' in k}
    dic = {k: v for k, v in dic.items() if '#NT' not in k}
    dic = {k: v for k, v in dic.items() if k.split('#')[0] not in stop_list}
    return sort_dic(dic, sort_key=1, rev=True)


def extract_noun(st):
    return {x for x in st if '#N' in x}


def convert_bag_of_words(df, col, kl):
    """
    Convert a Series of parsed series in a df to a word matrix with a specific keyword list
    :param df: a pandas series
    :param col: the column to be converted
    :param kl: a specific keyword list
    :return: a word matrix
    """
    vectorizer = text.CountVectorizer(min_df=1, vocabulary=kl, token_pattern='(?u)\\b\\w+\\b')
    pp = df[col].tolist()
    matrix = vectorizer.fit_transform(pp)
    header = vectorizer.get_feature_names()
    text_matrix = pd.DataFrame(matrix.toarray(), columns=header)
    return text_matrix
